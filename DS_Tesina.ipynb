{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Spaces' Tesina <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Imports section<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import neighbors, model_selection, metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Read .csv file <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"dataR2.csv\"\n",
    "file = open(filename, \"r\")\n",
    "\n",
    "# data = np.loadtxt(file, delimiter=\",\", dtype=None, encoding=None, usecols=(0,1,2,3,4,5,6,7,8,9), skiprows=1)\n",
    "\n",
    "# according to the relevant papers indicated in the website where I took this dataset from has been has been\n",
    "# observed that if instead of taking into account all of the features we only take 4 of them (Age, BMI, Glucose\n",
    "# & Resistine) we can achieve a grater accuracy (only if we are using the SVM or at least Random Forest algos\n",
    "# whereas with KNN or Logistic regression results aren't that nice).\n",
    "\n",
    "data = np.loadtxt(file, delimiter=\",\", dtype=None, encoding=None, usecols=(0,1,2,7,9), skiprows=1)\n",
    "\n",
    "# records with the classification as \"1\" are Healthy Controls, \"2\" means Patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Preprocessing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide data into source and target (respectively X and Y)\n",
    "X = data[:, :-1]\n",
    "Y = data[:, len(data[0])-1]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, random_state=np.random.randint(0,100), test_size=0.3)\n",
    "\n",
    "# array of possible K to apply KNN neighbors\n",
    "ks = [3,5,7,9]\n",
    "\n",
    "\n",
    "# normalize data because algorithms work better with normalized data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I try to apply PCA on the dataset, since there are 9 features (which are a lot) and then re-apply KNN <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=4)\n",
    "# pca.fit(X)\n",
    "# Xp = pca.transform(X)\n",
    "# print(Xp)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = model_selection.train_test_split(Xp, Y, random_state=np.random.randint(0,100), test_size=0.3)\n",
    "\n",
    "# # array of possible K to apply KNN neighbors\n",
    "# ks = [3,5,7,9]\n",
    "\n",
    "\n",
    "# normalize data because algorithms work better with normalized data\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Apply K-NN</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test set with K = 3 is 0.686\n",
      "Accuracy score on the test set with K = 5 is 0.829\n",
      "Accuracy score on the test set with K = 7 is 0.686\n",
      "Accuracy score on the test set with K = 9 is 0.800\n"
     ]
    }
   ],
   "source": [
    "for k in ks:\n",
    "    n_neighbors = k\n",
    "    \n",
    "    # Create an instance of neighbors classifier (clf) and fit the data\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    \n",
    "    # train the classifier on the training set\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    print(\"Accuracy score on the test set with K =\",n_neighbors,\"is %.3f\" %(clf.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Apply KNN this time with a validation set </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best K, based on the validation set results, is 3 and the accuracy on the test set is 0.686\n"
     ]
    }
   ],
   "source": [
    "# Let's create the validation set\n",
    "X_train_t, X_valid, Y_train_t, Y_valid = model_selection.train_test_split(X_train, Y_train, random_state=np.random.randint(0,100), test_size=0.30)\n",
    "\n",
    "# array of possible K to apply KNN neighbors\n",
    "ks = [3,5,7,9]\n",
    "\n",
    "# normalize data because algorithms work better with normalized data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "# array to write accuracy values for each K \n",
    "acc_arr = []\n",
    "\n",
    "for k in ks:\n",
    "    n_neighbors = k\n",
    "    \n",
    "    # Create an instance of neighbors classifier (clf) and fit the data\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    \n",
    "    # train the classifier on the training set\n",
    "    clf.fit(X_train_t, Y_train_t)\n",
    "    \n",
    "    acc_arr.append(clf.score(X_valid, Y_valid))\n",
    "    \n",
    "# I choose the best K based on the results on the validation set and apply KNN with that\n",
    "k_best_index = acc_arr.index(max(acc_arr))\n",
    "k_best = ks[k_best_index]\n",
    "\n",
    "clf2 = neighbors.KNeighborsClassifier(k_best)\n",
    "clf2.fit(X_train_t, Y_train_t)\n",
    "\n",
    "print(\"The best K, based on the validation set results, is\",k_best,\"and the accuracy on the test set is %.3f\"%(clf2.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I'll try Logistic Regression on the dataset <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver=\"lbfgs\") # instance of the model\n",
    "\n",
    "logReg.fit(X_train, Y_train)\n",
    "\n",
    "# res = logReg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", logReg.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u> Naive Bayes classifier </u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6571428571428571\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(\"Accuracy: \", gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u> Random forest</u> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "rndFor = RandomForestClassifier(n_estimators = 200, criterion=\"entropy\")\n",
    "\n",
    "rndFor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = rndFor.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u> SVM</u> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(gamma='auto', kernel='rbf')\n",
    "svc.fit(X_train, Y_train)\n",
    "print(\"Accuracy: \", svc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> So this is the accuracy with the default parameters, but with SVm approach is necessary to perform some parameters tuning in order to achieve a better result. To do so I relied on the GridSearch <p>\n",
    "<p><u> SVM with GridSearch </u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1], 'kernel': ['rbf']}]\n",
    "svcGS = GridSearchCV(svc, parameters, n_jobs=-1, cv=5)\n",
    "svcGS.fit(X_train, Y_train)\n",
    "print(\"Accuracy: \", svcGS.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
