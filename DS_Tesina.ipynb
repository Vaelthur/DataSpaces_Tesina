{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Spaces' Tesina <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Imports section<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import neighbors, model_selection, metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Read .csv file <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"dataR2.csv\"\n",
    "file = open(filename, \"r\")\n",
    "\n",
    "# data = np.loadtxt(file, delimiter=\",\", dtype=None, encoding=None, usecols=(0,1,2,3,4,5,6,7,8,9), skiprows=1)\n",
    "\n",
    "# according to the relevant papers indicated in the website where I took this dataset from has been has been\n",
    "# observed that if instead of taking into account all of the features we only take 4 of them (Age, BMI, Glucose\n",
    "# & Resistine) we can achieve a grater accuracy (only if we are using the SVM or at least Random Forest algos\n",
    "# whereas with KNN or Logistic regression results aren't that nice).\n",
    "\n",
    "data = np.loadtxt(file, delimiter=\",\", dtype=None, encoding=None, usecols=(0,1,2,7,9), skiprows=1)\n",
    "\n",
    "# records with the classification as \"1\" are Healthy Controls, \"2\" means Patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Preprocessing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide data into source and target (respectively X and Y)\n",
    "X = data[:, :-1]\n",
    "Y = data[:, len(data[0])-1]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, random_state=np.random.randint(0,100), test_size=0.3)\n",
    "\n",
    "# array of possible K to apply KNN neighbors\n",
    "ks = [3,5,7,9]\n",
    "\n",
    "\n",
    "# normalize data because algorithms work better with normalized data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I try to apply PCA on the dataset, since there are 9 features (which are a lot) and then re-apply KNN <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=4)\n",
    "# pca.fit(X)\n",
    "# Xp = pca.transform(X)\n",
    "# print(Xp)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = model_selection.train_test_split(Xp, Y, random_state=np.random.randint(0,100), test_size=0.3)\n",
    "\n",
    "# # array of possible K to apply KNN neighbors\n",
    "# ks = [3,5,7,9]\n",
    "\n",
    "\n",
    "# normalize data because algorithms work better with normalized data\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Apply K-NN</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test set with K = 3 is 0.800\n",
      "Accuracy score on the test set with K = 5 is 0.771\n",
      "Accuracy score on the test set with K = 7 is 0.829\n",
      "Accuracy score on the test set with K = 9 is 0.714\n"
     ]
    }
   ],
   "source": [
    "for k in ks:\n",
    "    n_neighbors = k\n",
    "    \n",
    "    # Create an instance of neighbors classifier (clf) and fit the data\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    \n",
    "    # train the classifier on the training set\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    print(\"Accuracy score on the test set with K =\",n_neighbors,\"is %.3f\" %(clf.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Apply KNN this time with a validation set </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best K, based on the validation set results, is 3 and the accuracy on the test set is 0.686\n"
     ]
    }
   ],
   "source": [
    "# Let's create the validation set\n",
    "X_train_t, X_valid, Y_train_t, Y_valid = model_selection.train_test_split(X_train, Y_train, random_state=np.random.randint(0,100), test_size=0.30)\n",
    "\n",
    "# array of possible K to apply KNN neighbors\n",
    "ks = [3,5,7,9]\n",
    "\n",
    "# normalize data because algorithms work better with normalized data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "# array to write accuracy values for each K \n",
    "acc_arr = []\n",
    "\n",
    "for k in ks:\n",
    "    n_neighbors = k\n",
    "    \n",
    "    # Create an instance of neighbors classifier (clf) and fit the data\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    \n",
    "    # train the classifier on the training set\n",
    "    clf.fit(X_train_t, Y_train_t)\n",
    "    \n",
    "    acc_arr.append(clf.score(X_valid, Y_valid))\n",
    "    \n",
    "# I choose the best K based on the results on the validation set and apply KNN with that\n",
    "k_best_index = acc_arr.index(max(acc_arr))\n",
    "k_best = ks[k_best_index]\n",
    "\n",
    "clf2 = neighbors.KNeighborsClassifier(k_best)\n",
    "clf2.fit(X_train_t, Y_train_t)\n",
    "\n",
    "print(\"The best K, based on the validation set results, is\",k_best,\"and the accuracy on the test set is %.3f\"%(clf2.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I'll try Logistic Regression on the dataset <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.93      0.84        14\n",
      "         2.0       0.94      0.81      0.87        21\n",
      "\n",
      "    accuracy                           0.86        35\n",
      "   macro avg       0.85      0.87      0.86        35\n",
      "weighted avg       0.87      0.86      0.86        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver=\"lbfgs\") # instance of the model\n",
    "\n",
    "logReg.fit(X_train, Y_train)\n",
    "\n",
    "# res = logReg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", logReg.score(X_test, Y_test))\n",
    "print(classification_report(Y_test, logReg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u> Naive Bayes classifier </u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(\"Accuracy: \", gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u> Random forest</u> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "rndFor = RandomForestClassifier(n_estimators = 200, criterion=\"entropy\")\n",
    "\n",
    "rndFor.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = rndFor.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><u> SVM</u> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(gamma='auto', kernel='rbf')\n",
    "svc.fit(X_train, Y_train)\n",
    "print(\"Accuracy: \", svc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> So this is the accuracy with the default parameters, but with SVm approach is necessary to perform some parameters tuning in order to achieve a better result. To do so I relied on the GridSearch <p>\n",
    "<p><u> SVM with GridSearch </u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.86      0.77        14\n",
      "         2.0       0.89      0.76      0.82        21\n",
      "\n",
      "    accuracy                           0.80        35\n",
      "   macro avg       0.80      0.81      0.80        35\n",
      "weighted avg       0.82      0.80      0.80        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'C': [0.1, 0.2, 1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [0.1, 0.2, 1, 10, 100, 1000], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.25, 1], 'kernel': ['rbf']}]\n",
    "svcGS = GridSearchCV(svc, parameters, n_jobs=-1, cv=5, refit=True)\n",
    "svcGS.fit(X_train, Y_train)\n",
    "# print(\"Accuracy: \", svcGS.score(X_test, Y_test))\n",
    "print(classification_report(Y_test, svcGS.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The result above shows us a fuller report, from which we can observe that, even if the total accuracy (f1 score) is 0.8 what we are more interested in (in this specific case) is the precision for the class 2.0, that is the class that indicates the patients tested positive for breast cancer. \n",
    "From here we can say that if a patient is positive for breast cancer this final model we can tell correctly the shes is in almost 90% of the cases </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
